{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sklearn.ensemble\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "import xaibenchmark as xb\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure you have adult/adult.data inside dataset_folder\n",
    "dataset_folder = '../data/'\n",
    "adult_dataset = utils.load_dataset('adult', balance=True, dataset_folder=dataset_folder, discretize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train 0.9350338780390594\nTest 0.8489483747609943\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=50, n_jobs=5)\n",
    "rf.fit(adult_dataset.train, adult_dataset.labels_train)\n",
    "print('Train', sklearn.metrics.accuracy_score(adult_dataset.labels_train, rf.predict(adult_dataset.train)))\n",
    "print('Test', sklearn.metrics.accuracy_score(adult_dataset.labels_test, rf.predict(adult_dataset.test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Implementation of Anchors Explainer onto base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class AnchorsExplainer(xb.Explainer):\n",
    "    \"\"\"\n",
    "    implementation of the Explainer \"Anchors\" onto the base explainer class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predictor, dataset):       \n",
    "        self.explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "            dataset.class_names,\n",
    "            dataset.feature_names,\n",
    "            dataset.train,\n",
    "            dataset.categorical_names)\n",
    "        self.dataset = dataset\n",
    "        self.predictor = predictor\n",
    "\n",
    "    def get_subset(self, subset_name):\n",
    "        \"\"\"\n",
    "        Returns one of the 3 subsets given a name\n",
    "        :param subset_name: either train, dev or test\n",
    "        :return: data subset as ndarray\n",
    "        \"\"\"\n",
    "        if subset_name == \"train\":\n",
    "            return self.dataset.train, self.dataset.labels_train\n",
    "        elif subset_name == \"dev\":\n",
    "            return self.dataset.validation, self.dataset.labels_validation\n",
    "        elif subset_name == \"test\":\n",
    "            return self.dataset.test, self.dataset.labels_test\n",
    "        else:\n",
    "            raise NameError(\"This subset name is not one of train, dev, test.\")\n",
    "        \n",
    "    def explain_instance(self, instance, instance_set, threshold=0.95):\n",
    "        \"\"\"\n",
    "        Creates an Anchor explanation based on a given instance\n",
    "        :param instance: \"Anchor\" for explanation\n",
    "        :param instance_set: textual information about subset for metric information\n",
    "        :param threshold: Worst possible precision for the explanation\n",
    "        :return: the explanation\n",
    "        \"\"\"\n",
    "        self.explanation = self.explainer.explain_instance(instance, self.predictor.predict, threshold=threshold)\n",
    "        self.instance = instance\n",
    "        self.instance_set, self.instance_label_set = self.get_subset(instance_set)\n",
    "        return self.explanation\n",
    "    \n",
    "    @xb.metric\n",
    "    def coverage(self):\n",
    "        \"\"\"\n",
    "        The relative amount of data elements that are in the area of the explanation\n",
    "        :return: the coverage value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.coverage()\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def precision(self):\n",
    "        \"\"\"\n",
    "        The ML-accuracy of the explanation when applied to the whole dataset (not just the area of the explanation)\n",
    "        :return: the precision value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.precision()\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_explanation(self):\n",
    "        \"\"\"\n",
    "        New implementation of balance:\n",
    "        Relative amount of data elements in the explanation neighborhood that had an assigned label value of 1\n",
    "        (by the explanation)\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        # balance is always 0 or 1 because Anchors creates a neighborhood where all elements are supposed to have \n",
    "        # the same label as the one that was used to instantiate the explanation\n",
    "        return self.explanation.exp_map[\"prediction\"]\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_train(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the training set with a label value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.train[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_train[fit_anchor])\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_dev(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the dev set with a label value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.validation[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_validation[fit_anchor])\n",
    "        return np.nan  \n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_test(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the test set with a label value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.test[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_test[fit_anchor])\n",
    "        return np.nan\n",
    "            \n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_train(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the neighborhood of the explanation in the training set\n",
    "        with an assigned label (by the ML model) value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.train[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.predictor.predict(self.dataset.train[fit_anchor]))\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_dev(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the neighborhood of the explanation in the dev set\n",
    "        with an assigned label (by the ML model) value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.validation[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.predictor.predict(self.dataset.validation[fit_anchor]))\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_test(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the neighborhood of the explanation in the test set\n",
    "        with an assigned label (by the ML model) value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.test[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.predictor.predict(self.dataset.test[fit_anchor]))\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def area(self):\n",
    "        \"\"\"\n",
    "        Relative amount of feature space over all features n that is specified by the explanation.\n",
    "        area = Product[i=1->n] fi, f: 1 if feature is not in explanation, else 1/m, m: deminsionality of feature\n",
    "        :return: the area value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            array = np.amax(self.dataset.train, axis=0)[self.explanation.features()]\n",
    "            array = array + 1\n",
    "            # 1/4*1*1*1*1/10 = 2.5%\n",
    "            # optionally with n-th root. n=amount of features or dimension of features?\n",
    "            # print(np.power(np.prod(1 / array), 1/len(array)), np.power(np.prod(1 / array), 1/np.sum(array)))\n",
    "            return np.prod(1 / array)\n",
    "        return np.nan\n",
    "    \n",
    "    # accuracy for all 3 subsets\n",
    "    @xb.metric\n",
    "    def accuracy_train(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in explanation neighborhood that have the same explanation label as\n",
    "        the label assigned by the ML model\n",
    "        :return: the accuracy value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            explanation_label = self.explanation.exp_map[\"prediction\"]\n",
    "            fit_anchor = np.where(np.all(self.dataset.train[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            relevant_examples = self.dataset.train[fit_anchor]\n",
    "            ml_pred = self.predictor.predict(relevant_examples)\n",
    "            return np.count_nonzero(ml_pred == explanation_label) / len(relevant_examples)\n",
    "        return np.nan                \n",
    "    \n",
    "    @xb.metric\n",
    "    def accuracy_dev(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in explanation neighborhood that have the same explanation label as\n",
    "        the label assigned by the ML model\n",
    "        :return: the accuracy value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            explanation_label = self.explanation.exp_map[\"prediction\"]\n",
    "            fit_anchor = np.where(np.all(self.dataset.validation[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            relevant_examples = self.dataset.validation[fit_anchor]\n",
    "            ml_pred = self.predictor.predict(relevant_examples)\n",
    "            return np.count_nonzero(ml_pred == explanation_label) / len(relevant_examples)\n",
    "        return np.nan \n",
    "    \n",
    "    @xb.metric\n",
    "    def accuracy_test(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in explanation neighborhood that have the same explanation label as\n",
    "        the label assigned by the ML model\n",
    "        :return: the accuracy value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            explanation_label = self.explanation.exp_map[\"prediction\"]\n",
    "            fit_anchor = np.where(np.all(self.dataset.test[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            relevant_examples = self.dataset.test[fit_anchor]\n",
    "            ml_pred = self.predictor.predict(relevant_examples)\n",
    "            return np.count_nonzero(ml_pred == explanation_label) / len(relevant_examples)\n",
    "        return np.nan \n",
    "    \n",
    "    @xb.utility\n",
    "    def get_neighborhood_instances(self):\n",
    "        \"\"\"\n",
    "        Receive all data elements in the given subset that belong to the neighborhood of the explanation\n",
    "        :return: ndarray of elements\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.instance_set[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return self.instance_set[fit_anchor]\n",
    "        return []\n",
    "    \n",
    "    @xb.utility\n",
    "    def get_explained_instance(self):\n",
    "        return self.instance\n",
    "    \n",
    "    @xb.utility\n",
    "    def distance(self, x, y):\n",
    "        return np.linalg.norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage of implemented explainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# instantiate anchors explainer\n",
    "exp = AnchorsExplainer(rf, adult_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Current explanation: ['Education = Doctorate', 'Relationship = Husband']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "explanation = exp.explain_instance(exp.dataset.test[101], \"test\", threshold=0.9)\n",
    "print(\"Current explanation:\", explanation.names())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy_dev',\n 'accuracy_test',\n 'accuracy_train',\n 'area',\n 'balance_data_dev',\n 'balance_data_test',\n 'balance_data_train',\n 'balance_explanation',\n 'balance_model_dev',\n 'balance_model_test',\n 'balance_model_train',\n 'coverage',\n 'furthest_distance',\n 'inverse_coverage',\n 'precision']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ],
   "source": [
    "# get all currently defined metrics\n",
    "exp.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{('accuracy_dev', 1.0),\n ('accuracy_test', 1.0),\n ('accuracy_train', 0.9659090909090909),\n ('area', 0.010416666666666666),\n ('balance_data_dev', 0.9565217391304348),\n ('balance_data_test', 0.9090909090909091),\n ('balance_data_train', 0.9261363636363636),\n ('balance_explanation', 1),\n ('balance_model_dev', 1.0),\n ('balance_model_test', 1.0),\n ('balance_model_train', 0.9659090909090909),\n ('coverage', 0.0132),\n ('furthest_distance', 39.21734310225516),\n ('inverse_coverage', 75.75757575757576),\n ('precision', 0.9177700348432056)}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "# report all current metrics\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "inferred metrics: {'precision', 'accuracy_train', 'balance_data_test', 'coverage', 'accuracy_dev', 'inverse_coverage', 'furthest_distance', 'balance_model_test', 'balance_model_train', 'balance_data_train', 'accuracy_test', 'area', 'balance_data_dev', 'balance_explanation', 'balance_model_dev'}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# infer other possible metrics\n",
    "exp.infer_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}