{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sklearn.ensemble\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "import xaibenchmark as xb\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure you have adult/adult.data inside dataset_folder\n",
    "dataset_folder = '../data/'\n",
    "adult_dataset = utils.load_dataset('adult', balance=True, dataset_folder=dataset_folder, discretize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train 0.9350338780390594\nTest 0.8489483747609943\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=50, n_jobs=5)\n",
    "rf.fit(adult_dataset.train, adult_dataset.labels_train)\n",
    "print('Train', sklearn.metrics.accuracy_score(adult_dataset.labels_train, rf.predict(adult_dataset.train)))\n",
    "print('Test', sklearn.metrics.accuracy_score(adult_dataset.labels_test, rf.predict(adult_dataset.test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Implementation of Anchors Explainer onto base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class AnchorsExplainer(xb.Explainer):\n",
    "    \"\"\"\n",
    "    implementation of the Explainer \"Anchors\" onto the base explainer class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predictor, dataset):       \n",
    "        self.explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "            dataset.class_names,\n",
    "            dataset.feature_names,\n",
    "            dataset.train,\n",
    "            dataset.categorical_names)\n",
    "        self.dataset = dataset\n",
    "        self.predictor = predictor\n",
    "\n",
    "    def get_subset(self, subset_name):\n",
    "        \"\"\"\n",
    "        Returns one of the 3 subsets given a name\n",
    "        :param subset_name: either train, dev or test\n",
    "        :return: data subset as ndarray\n",
    "        \"\"\"\n",
    "        if subset_name == \"train\":\n",
    "            return self.dataset.train, self.dataset.labels_train\n",
    "        elif subset_name == \"dev\":\n",
    "            return self.dataset.validation, self.dataset.labels_validation\n",
    "        elif subset_name == \"test\":\n",
    "            return self.dataset.test, self.dataset.labels_test\n",
    "        else:\n",
    "            raise NameError(\"This subset name is not one of train, dev, test.\")\n",
    "        \n",
    "    def explain_instance(self, instance, instance_set, threshold=0.95):\n",
    "        \"\"\"\n",
    "        Creates an Anchor explanation based on a given instance\n",
    "        :param instance: \"Anchor\" for explanation\n",
    "        :param instance_set: textual information about subset for metric information\n",
    "        :param threshold: Worst possible precision for the explanation\n",
    "        :return: the explanation\n",
    "        \"\"\"\n",
    "        self.explanation = self.explainer.explain_instance(instance, self.predictor.predict, threshold=threshold)\n",
    "        self.instance = instance\n",
    "        self.instance_set, self.instance_label_set = self.get_subset(instance_set)\n",
    "        return self.explanation\n",
    "    \n",
    "    @xb.metric\n",
    "    def coverage(self):\n",
    "        \"\"\"\n",
    "        The relative amount of data elements that are in the area of the explanation\n",
    "        :return: the coverage value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.coverage()\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def precision(self):\n",
    "        \"\"\"\n",
    "        The ML-accuracy of the explanation when applied to the whole dataset (not just the area of the explanation)\n",
    "        :return: the precision value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.precision()\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_explanation(self):\n",
    "        \"\"\"\n",
    "        New implementation of balance:\n",
    "        Relative amount of data elements in the explanation neighborhood that had an assigned label value of 1\n",
    "        (by the explanation)\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        # balance is always 0 or 1 because Anchors creates a neighborhood where all elements are supposed to have \n",
    "        # the same label as the one that was used to instantiate the explanation\n",
    "        return self.explanation.exp_map[\"prediction\"]\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_train(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the training set with a label value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.train[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_train[fit_anchor])\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_dev(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the dev set with a label value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.validation[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_validation[fit_anchor])\n",
    "        return np.nan  \n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_test(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the test set with a label value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.test[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_test[fit_anchor])\n",
    "        return np.nan\n",
    "            \n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_train(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the neighborhood of the explanation in the training set\n",
    "        with an assigned label (by the ML model) value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.train[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.predictor.predict(self.dataset.train[fit_anchor]))\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_dev(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the neighborhood of the explanation in the dev set\n",
    "        with an assigned label (by the ML model) value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.validation[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.predictor.predict(self.dataset.validation[fit_anchor]))\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_test(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in the neighborhood of the explanation in the test set\n",
    "        with an assigned label (by the ML model) value of 1\n",
    "        :return: the balance value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.test[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.predictor.predict(self.dataset.test[fit_anchor]))\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def area(self):\n",
    "        \"\"\"\n",
    "        Relative amount of feature space over all features n that is specified by the explanation.\n",
    "        area = Product[i=1->n] fi, f: 1 if feature is not in explanation, else 1/m, m: deminsionality of feature\n",
    "        :return: the area value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            array = np.amax(self.dataset.train, axis=0)[self.explanation.features()]\n",
    "            array = array + 1\n",
    "            \n",
    "            # optionally with n-th root. n=amount of features or dimension of features?\n",
    "            # print(np.power(np.prod(1 / array), 1/len(array)), np.power(np.prod(1 / array), 1/np.sum(array)))\n",
    "            return np.prod(1 / array)\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def accuracy(self):\n",
    "        \"\"\"\n",
    "        Relative amount of data elements in explanation neighborhood that have the same explanation label as\n",
    "        the label assigned by the ML model\n",
    "        :return: the accuracy value\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            explanation_label = self.explanation.exp_map[\"prediction\"]\n",
    "            relevant_examples = self.get_neighborhood_instances()\n",
    "            ml_pred = self.predictor.predict(relevant_examples)\n",
    "            return np.count_nonzero(ml_pred == explanation_label) / len(relevant_examples)\n",
    "        return np.nan                \n",
    "    \n",
    "    @xb.utility\n",
    "    def get_neighborhood_instances(self):\n",
    "        \"\"\"\n",
    "        Receive all data elements in the given subset that belong to the neighborhood of the explanation\n",
    "        :return: ndarray of elements\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.instance_set[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return self.instance_set[fit_anchor]\n",
    "        return []\n",
    "    \n",
    "    @xb.utility\n",
    "    def get_explained_instance(self):\n",
    "        return self.instance\n",
    "    \n",
    "    @xb.utility\n",
    "    def distance(self, x, y):\n",
    "        return np.linalg.norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage of implemented explainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# instantiate anchors explainer\n",
    "exp = AnchorsExplainer(rf, adult_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Current explanation: ['Marital Status = Married-spouse-absent']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "explanation = exp.explain_instance(exp.dataset.test[8], \"test\", threshold=0.65)\n",
    "print(\"Current explanation:\", explanation.names())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'area',\n 'balance_data_dev',\n 'balance_data_test',\n 'balance_data_train',\n 'balance_explanation',\n 'balance_model_dev',\n 'balance_model_test',\n 'balance_model_train',\n 'coverage',\n 'precision']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 27
    }
   ],
   "source": [
    "# get all currently defined metrics\n",
    "exp.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{('accuracy', 0.8095238095238095),\n ('area', 0.14285714285714285),\n ('balance_data_dev', 0.2631578947368421),\n ('balance_data_test', 0.23809523809523808),\n ('balance_data_train', 0.2072072072072072),\n ('balance_explanation', 0),\n ('balance_model_dev', 0.21052631578947367),\n ('balance_model_test', 0.19047619047619047),\n ('balance_model_train', 0.2072072072072072),\n ('coverage', 0.0096),\n ('precision', 0.6595289079229122)}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 28
    }
   ],
   "source": [
    "# report all current metrics\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "inferred metrics: {'furthest_distance', 'balance_model_train', 'precision', 'area', 'balance_data_train', 'balance_model_test', 'balance_data_test', 'balance_model_dev', 'coverage', 'balance_explanation', 'inverse_coverage', 'accuracy', 'balance_data_dev'}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# infer other possible metrics\n",
    "exp.infer_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}