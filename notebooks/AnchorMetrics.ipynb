{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sklearn.ensemble\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "import xaibenchmark as xb\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure you have adult/adult.data inside dataset_folder\n",
    "dataset_folder = '../data/'\n",
    "dataset = utils.load_dataset('adult', balance=True, dataset_folder=dataset_folder, discretize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train 0.9350338780390594\nTest 0.8489483747609943\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=50, n_jobs=5)\n",
    "rf.fit(dataset.train, dataset.labels_train)\n",
    "print('Train', sklearn.metrics.accuracy_score(dataset.labels_train, rf.predict(dataset.train)))\n",
    "print('Test', sklearn.metrics.accuracy_score(dataset.labels_test, rf.predict(dataset.test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Implementation of Anchors Explainer onto base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class AnchorsExplainer(xb.Explainer):\n",
    "    \n",
    "    def __init__(self, predictor, dataset):\n",
    "        \n",
    "        self.explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "            dataset.class_names,\n",
    "            dataset.feature_names,\n",
    "            dataset.train,\n",
    "            dataset.categorical_names)\n",
    "        self.dataset = dataset\n",
    "        self.predictor = predictor\n",
    "\n",
    "    def get_subset(self, subset_name):\n",
    "        if subset_name == \"train\":\n",
    "            return self.dataset.train, self.dataset.labels_train\n",
    "        elif subset_name == \"dev\":\n",
    "            return self.dataset.validation, self.dataset.labels_validation\n",
    "        elif subset_name == \"test\":\n",
    "            return self.dataset.test, self.dataset.labels_test\n",
    "        else:\n",
    "            raise NameError\n",
    "        \n",
    "    def explain_instance(self, instance, instance_set, threshold=0.95):\n",
    "        self.explanation = self.explainer.explain_instance(instance, self.predictor.predict, threshold=threshold)\n",
    "        self.instance = instance\n",
    "        self.instance_set, self.instance_label_set = self.get_subset(instance_set)\n",
    "        return self.explanation\n",
    "    \n",
    "    @xb.metric\n",
    "    def coverage(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.coverage()\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def precision(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.precision()\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_train(self):\n",
    "        return np.mean(self.dataset.labels_train)\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_dev(self):\n",
    "        return np.mean(self.dataset.labels_validation)    \n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_data_test(self):\n",
    "        return np.mean(self.dataset.labels_test)\n",
    "            \n",
    "    @xb.metric\n",
    "    def balance_explanation_train(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.train[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_train[fit_anchor])\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_explanation_dev(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.validation[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_validation[fit_anchor])\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_explanation_test(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.dataset.test[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return np.mean(self.dataset.labels_test[fit_anchor])\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_train(self):\n",
    "        return np.mean(self.predictor.predict(self.dataset.train))\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_dev(self):\n",
    "        return np.mean(self.predictor.predict(self.dataset.validation))\n",
    "    \n",
    "    @xb.metric\n",
    "    def balance_model_test(self):\n",
    "        return np.mean(self.predictor.predict(self.dataset.test))\n",
    "    \n",
    "    @xb.metric\n",
    "    def area(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            array = np.amax(self.dataset.train, axis=0)[self.explanation.features()]\n",
    "            array = array + 1\n",
    "            \n",
    "            # optionally with n-th root. n=amount of features or dimension of features?\n",
    "            # print(np.power(np.prod(1 / array), 1/len(array)), np.power(np.prod(1 / array), 1/np.sum(array)))\n",
    "            return np.prod(1 / array)\n",
    "        return np.nan\n",
    "    \n",
    "    @xb.metric\n",
    "    def accuracy(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            explanation_label = self.explanation.exp_map[\"prediction\"]\n",
    "            relevant_examples = self.get_neighborhood_instances()\n",
    "            ml_pred = self.predictor.predict(relevant_examples)\n",
    "            return np.count_nonzero(ml_pred == explanation_label) / len(relevant_examples)\n",
    "        return np.nan                \n",
    "    \n",
    "    @xb.utility\n",
    "    def get_neighborhood_instances(self): \n",
    "        if hasattr(self, 'explanation'):\n",
    "            fit_anchor = np.where(np.all(self.instance_set[:, self.explanation.features()] == \n",
    "                                         self.instance[self.explanation.features()], axis=1))[0]\n",
    "            return self.instance_set[fit_anchor]\n",
    "        return []\n",
    "    \n",
    "    @xb.utility\n",
    "    def get_explained_instance(self):\n",
    "        return self.instance\n",
    "    \n",
    "    @xb.utility\n",
    "    def distance(self, x, y):\n",
    "        return np.linalg.norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage of implemented explainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['Education = Doctorate']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# instantiate anchors explainer\n",
    "exp = AnchorsExplainer(rf, dataset)\n",
    "explanation = exp.explain_instance(dataset.test[5], \"test\", threshold=0.6)\n",
    "print(\"Current explanation:\", explanation.names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['accuracy',\n 'area',\n 'balance_data_dev',\n 'balance_data_test',\n 'balance_data_train',\n 'balance_explanation_dev',\n 'balance_explanation_test',\n 'balance_explanation_train',\n 'balance_model_dev',\n 'balance_model_test',\n 'balance_model_train',\n 'coverage',\n 'precision']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 141
    }
   ],
   "source": [
    "# get all currently defined metrics\n",
    "exp.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Neighborhood:  [[ 3.  4. 10.  2. 10.  0.  4.  1.  0.  0.  2. 39.]\n [ 3.  1. 10.  2. 10.  0.  4.  1.  2.  0.  2. 11.]\n [ 2.  4. 10.  2. 10.  0.  4.  1.  0.  0.  2. 39.]\n [ 3.  4. 10.  2. 10.  0.  4.  1.  0.  0.  0. 39.]\n [ 1.  1. 10.  2. 10.  0.  4.  1.  0.  0.  2.  0.]\n [ 2.  4. 10.  2. 10.  5.  4.  0.  0.  0.  0. 39.]\n [ 2.  4. 10.  3.  8.  1.  4.  1.  2.  0.  2.  0.]\n [ 3.  7. 10.  2. 10.  0.  4.  1.  0.  0.  0. 39.]\n [ 1.  4. 10.  0. 10.  1.  4.  0.  0.  0.  2. 39.]\n [ 2.  5. 10.  2. 10.  0.  4.  1.  0.  0.  1. 39.]\n [ 2.  4. 10.  2. 10.  0.  4.  1.  0.  0.  2. 39.]\n [ 2.  4. 10.  4. 10.  1.  4.  1.  0.  0.  1. 39.]\n [ 2.  4. 10.  2. 10.  0.  4.  1.  0.  0.  1. 39.]\n [ 2.  6. 10.  4. 10.  1.  4.  0.  0.  0.  1. 39.]\n [ 1.  4. 10.  2. 10.  0.  4.  1.  0.  0.  2. 39.]\n [ 3.  7. 10.  4.  4.  1.  4.  0.  0.  0.  1. 39.]\n [ 3.  6. 10.  2. 12.  0.  4.  1.  0.  2.  0. 39.]\n [ 3.  7. 10.  2.  4.  0.  4.  1.  0.  2.  2. 39.]\n [ 1.  6. 10.  2. 10.  0.  4.  1.  0.  0.  2. 39.]\n [ 3.  4. 10.  2.  4.  0.  4.  1.  0.  2.  1. 39.]\n [ 2.  4. 10.  2.  4.  5.  4.  0.  2.  0.  0. 39.]\n [ 3.  4. 10.  2. 10.  0.  4.  1.  0.  0.  2. 39.]\n [ 3.  5. 10.  4. 10.  1.  4.  0.  0.  0.  2.  0.]\n [ 3.  7. 10.  0. 10.  4.  4.  1.  2.  0.  1. 39.]\n [ 1.  4. 10.  2. 10.  5.  4.  0.  0.  0.  2. 39.]\n [ 3.  7. 10.  0. 10.  1.  4.  1.  0.  0.  2. 39.]\n [ 3.  6. 10.  2. 10.  5.  4.  0.  0.  0.  0. 39.]\n [ 2.  5. 10.  2. 10.  0.  4.  1.  2.  0.  2. 39.]\n [ 1.  1. 10.  2. 10.  0.  4.  1.  0.  0.  2.  0.]\n [ 2.  6. 10.  4. 10.  4.  4.  0.  0.  0.  2. 39.]\n [ 3.  6. 10.  2. 12.  0.  4.  1.  0.  2.  0. 39.]\n [ 2.  5. 10.  2. 10.  0.  4.  1.  0.  0.  2. 39.]\n [ 3.  4. 10.  2. 10.  0.  4.  1.  0.  0.  0.  3.]\n [ 3.  0. 10.  2.  0.  0.  4.  1.  0.  0.  0. 39.]\n [ 1.  6. 10.  2. 10.  0.  4.  1.  0.  0.  0.  2.]\n [ 2.  4. 10.  2.  4.  5.  4.  0.  2.  0.  0. 39.]]\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "{('accuracy', 0.9166666666666666),\n ('area', 0.0625),\n ('balance_data_dev', 0.4968112244897959),\n ('balance_data_test', 0.49776927979604846),\n ('balance_data_train', 0.5006775607811877),\n ('balance_explanation_dev', 0.896551724137931),\n ('balance_explanation_test', 0.8333333333333334),\n ('balance_explanation_train', 0.9026217228464419),\n ('balance_model_dev', 0.5280612244897959),\n ('balance_model_test', 0.5264499681325685),\n ('balance_model_train', 0.5195695496213631),\n ('coverage', 0.0222),\n ('precision', 0.6305705955851728)}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 142
    }
   ],
   "source": [
    "# report all current metrics\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "inferred metrics: {'balance_data_test', 'accuracy', 'balance_data_train', 'balance_model_dev', 'balance_model_train', 'inverse_coverage', 'area', 'balance_explanation_test', 'furthest_distance', 'coverage', 'precision', 'balance_model_test', 'balance_data_dev', 'balance_explanation_dev', 'balance_explanation_train'}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# infer other possible metrics\n",
    "exp.infer_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}