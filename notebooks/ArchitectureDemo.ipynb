{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Set up a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have adult/adult.data inside dataset_folder\n",
    "dataset_folder = '../data/'\n",
    "dataset = utils.load_dataset('adult', balance=True, dataset_folder=dataset_folder, discretize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0.9350338780390594\n",
      "Test 0.8489483747609943\n"
     ]
    }
   ],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=50, n_jobs=5)\n",
    "rf.fit(dataset.train, dataset.labels_train)\n",
    "print('Train', sklearn.metrics.accuracy_score(dataset.labels_train, rf.predict(dataset.train)))\n",
    "print('Test', sklearn.metrics.accuracy_score(dataset.labels_test, rf.predict(dataset.test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Set up the Explainer Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(fn):\n",
    "    # mark the method as something that requires view's class\n",
    "    fn.tag = 'metric'\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @metric\n",
    "    def area(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @metric\n",
    "    def coverage(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def metrics(self):\n",
    "                \n",
    "        def checkImplemented(f):\n",
    "            try:\n",
    "                f()\n",
    "            except NotImplementedError:\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        all_metrics_strings = [x for x in dir(self) if getattr(getattr(self, x), 'tag', None) == 'metric']\n",
    "        all_metrics = [getattr(self, m) for m in all_metrics_strings]\n",
    "        implemented_metrics = [metric for metric, metric_name in zip(all_metrics, all_metrics_strings) if checkImplemented(metric)]\n",
    "        \n",
    "        implemented_metric_names = set([metric_name for metric, metric_name in zip(all_metrics, all_metrics_strings) if checkImplemented(metric)])\n",
    "        return implemented_metric_names\n",
    "    \n",
    "    def infer_metrics(self):\n",
    "        \n",
    "        def checkImplemented(f):\n",
    "            try:\n",
    "                f()\n",
    "            except NotImplementedError:\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        all_metrics_strings = [x for x in dir(self) if getattr(getattr(self, x), 'tag', None) == 'metric']\n",
    "        all_metrics = [getattr(self, m) for m in all_metrics_strings]\n",
    "        implemented_metrics = {metric for metric, metric_name in zip(all_metrics, all_metrics_strings) if checkImplemented(metric)}\n",
    "        implemented_metric_names = set([metric_name for metric, metric_name in zip(all_metrics, all_metrics_strings) if checkImplemented(metric)])\n",
    "        \n",
    "        transfer = [\n",
    "            ({'coverage'}, 'inverse_coverage', lambda : 1 / self.coverage()),\n",
    "        ]\n",
    "        \n",
    "        old_metrics = {}\n",
    "        new_metrics = implemented_metric_names\n",
    "        while (new_metrics != old_metrics):\n",
    "            for transfer_list in transfer:\n",
    "                if transfer_list[0] <= new_metrics:\n",
    "                    setattr(self, transfer_list[1], metric(transfer_list[2]))\n",
    "                    \n",
    "            old_metrics = new_metrics\n",
    "            all_metrics_strings = [x for x in dir(self) if getattr(getattr(self, x), 'tag', None) == 'metric']\n",
    "            all_metrics = [getattr(self, m) for m in all_metrics_strings]\n",
    "            new_metrics = {metric_name for metric, metric_name in zip(all_metrics, all_metrics_strings) if checkImplemented(metric)}\n",
    "            \n",
    "        print('inferred metrics:', new_metrics)\n",
    "        \n",
    "    def report(self):\n",
    "        \n",
    "        def checkImplemented(f):\n",
    "            try:\n",
    "                f()\n",
    "            except NotImplementedError:\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        all_metrics = {(x, getattr(self, x)) for x in dir(self) if getattr(getattr(self, x), 'tag', None) == 'metric'}\n",
    "        implemented_metrics = {(x, f()) for (x, f) in all_metrics if checkImplemented(f)}\n",
    "        return implemented_metrics\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "# Set up specific Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimeExplainer(Explainer):\n",
    "    \n",
    "    def __init__(self, data, feature_names, class_names):\n",
    "        self.explainer = lime.lime_tabular.LimeTabularExplainer(data, feature_names=feature_names,\n",
    "                                                   class_names=class_names,\n",
    "                                                   discretize_continuous=False)\n",
    "        self.training_data = data\n",
    "        \n",
    "    @metric\n",
    "    def coverage(self):\n",
    "        return 6\n",
    "    \n",
    "    def distance(self):\n",
    "        return np.sqrt(training_data.shape[1]) * .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorsExplainer(Explainer):\n",
    "    \n",
    "    def __init__(self, class_names, feature_names, train, categorical_names):\n",
    "        \n",
    "        self.explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "            class_names,\n",
    "            feature_names,\n",
    "            train,\n",
    "            categorical_names)\n",
    "        \n",
    "    def explain_instance(self, instance, predictor, threshold=0.95):\n",
    "        self.explanation = self.explainer.explain_instance(instance, predictor, threshold=threshold)\n",
    "        return self.explanation\n",
    "    \n",
    "    @metric\n",
    "    def coverage(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.coverage()\n",
    "        raise NotImplementedError \n",
    "        \n",
    "    \n",
    "    @metric\n",
    "    def precision(self):\n",
    "        if hasattr(self, 'explanation'):\n",
    "            return self.explanation.precision()\n",
    "        raise NotImplementedError "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Use Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate anchors explainer\n",
    "exp = AnchorsExplainer(dataset.class_names,\n",
    "    dataset.feature_names,\n",
    "    dataset.train,\n",
    "    dataset.categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all currently defined metrics\n",
    "exp.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report all current metrics\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred metrics: set()\n"
     ]
    }
   ],
   "source": [
    "# infer other possible metrics\n",
    "exp.infer_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all currently defined metrics\n",
    "exp.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report all current metrics\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain a single instance (needed to compute coverage)\n",
    "explanation = exp.explain_instance(dataset.test[0], rf.predict, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage', 'precision'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all currently defined metrics\n",
    "exp.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('coverage', 0.0161), ('precision', 0.9833333333333333)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report all current metrics\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred metrics: {'inverse_coverage', 'coverage', 'precision'}\n"
     ]
    }
   ],
   "source": [
    "# infer other possible metrics\n",
    "exp.infer_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage', 'inverse_coverage', 'precision'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all currently defined metrics\n",
    "exp.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('coverage', 0.0161),\n",
       " ('inverse_coverage', 62.11180124223603),\n",
       " ('precision', 0.9833333333333333)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report all current metrics\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anchor",
   "language": "python",
   "name": "anchor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
