{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# util\n",
    "from xaibenchmark import load_adult as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = la.load_csv_data('adult', root_path='../data')\n",
    "\n",
    "def preprocess(*data_df): \n",
    "    def process_single(df):\n",
    "        \n",
    "        cat_df = pd.get_dummies(df, columns=data.categorical_features.keys())\n",
    "        missing_cols = {cat+'_'+str(attr) for cat in data.categorical_features \\\n",
    "                        for attr in data.categorical_features[cat]} - set(cat_df.columns)\n",
    "        for c in missing_cols:\n",
    "            cat_df[c] = 0\n",
    "            \n",
    "        cont_idx = list(set(data.data.keys()) - set(data.categorical_features.keys()))\n",
    "        cat_idx = [cat+'_'+str(attr) for cat in data.categorical_features \\\n",
    "                   for attr in data.categorical_features[cat]]\n",
    "        idx = cont_idx + cat_idx\n",
    "        return cat_df[idx]\n",
    "        \n",
    "    # Preprocess function for one-hot encoding categorical data\n",
    "    return [process_single(df) for df in data_df]\n",
    "\n",
    "train, dev, test = preprocess(data.data, data.data_dev, data.data_test)\n",
    "labels_train, labels_dev, labels_test = data.target, data.target_dev, data.target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(n_estimators=500)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(train, labels_train.to_numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels_dev.to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.93      0.90      7436\n",
      "        >50K       0.72      0.61      0.66      2332\n",
      "\n",
      "    accuracy                           0.85      9768\n",
      "   macro avg       0.80      0.77      0.78      9768\n",
      "weighted avg       0.85      0.85      0.85      9768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report')\n",
    "print('{:->60}'.format(''))\n",
    "print(sklearn.metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xaibenchmark as xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimeExplainer(xb.Explainer):\n",
    "    \n",
    "    def __init__(self, train_data, train_labels, model, feature_names, target_names, discretize_continuous=True):\n",
    "        \n",
    "        self.explainer = lime.lime_tabular.LimeTabularExplainer(train_data, feature_names=feature_names,\n",
    "                                                   class_names=target_names,\n",
    "                                                   discretize_continuous=discretize_continuous)\n",
    "        self.train = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.model = model\n",
    "        self.kernel_width = np.sqrt(train_data.shape[1]) * .75\n",
    "        \n",
    "    def explain_instance(self, instance, predictor, num_features=10):\n",
    "        self.explanation = self.explainer.explain_instance(instance, predictor, num_features=num_features)\n",
    "        self.instance = instance\n",
    "        self.weighted_instances = self.get_weighted_instances()\n",
    "        return self.explanation\n",
    "    \n",
    "    @xb.metric\n",
    "    def area(self):\n",
    "        \"\"\"\n",
    "        Area that is covered by the kernel in high dimension of the feature space.\n",
    "        \"\"\"\n",
    "        kernel_width = np.sqrt(self.train.shape[1]) * .75\n",
    "        kernel_dimension = self.train.shape[1]\n",
    "        return (kernel_width * np.sqrt(2*np.pi))**kernel_dimension\n",
    "    \n",
    "    @xb.metric\n",
    "    def coverage(self):\n",
    "        \"\"\"\n",
    "        Proportion of instances covered in the area\n",
    "        \"\"\"\n",
    "        weighted_instances = self.weighted_instances\n",
    "        return sum([weight for _, _, weight in weighted_instances]) / len(weighted_instances)\n",
    "    \n",
    "    @xb.metric\n",
    "    def furthest_distance(self):\n",
    "        kernel_width = np.sqrt(self.train.shape[1]) * .75\n",
    "        def kernel(distance):\n",
    "            return np.sqrt(np.exp(-distance**2/kernel_width**2))\n",
    "        training_instances = self.train.to_numpy()\n",
    "        distance_instances = (self.distance(self.instance, instance) for instance in training_instances)\n",
    "        weighted_distances = (distance * kernel(distance) for distance in distance_instances)\n",
    "        return sum(weighted_distances)\n",
    "\n",
    "    @xb.metric\n",
    "    def accuracy(self):\n",
    "        \"\"\"\n",
    "        Proportion of instances in the explanation neighborhood that shares the same output label by the\n",
    "        explainer and the ML model\n",
    "        \"\"\"\n",
    "\n",
    "        # loop through every instance from the neighborhood instances and calculate explain_instance to\n",
    "        # get the local_pred, which is a prediction from the explainer model (lime_base.py)\n",
    "        # (local_pred is the prediction of the explanation model on the original instance)\n",
    "\n",
    "        # e = exp.explain_instance(test.iloc[10], rf.predict_proba, num_features=10)\n",
    "        # e.local_pred[0]\n",
    "\n",
    "        # y_preds_exp\n",
    "        # y_preds_ml\n",
    "\n",
    "        pass\n",
    "\n",
    "    @xb.metric\n",
    "    def balance(self):\n",
    "        \"\"\"\n",
    "        Proportion of instances in the explanation neighborhood that has been assigned label 1 by the\n",
    "        explanation model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @xb.utility\n",
    "    def distance(self, x, y):\n",
    "        return np.linalg.norm(x-y)\n",
    "    \n",
    "    @xb.utility\n",
    "    def get_weighted_instances(self):     \n",
    "        if hasattr(self, 'explanation'):\n",
    "            kernel_width = np.sqrt(self.train.shape[1]) * .75\n",
    "            def kernel(distance):\n",
    "                return np.sqrt(np.exp(-distance**2/kernel_width**2))\n",
    "            return [(idx, instance, kernel(self.distance(self.instance, instance))) \\\n",
    "                    for idx, instance in enumerate(self.train.to_numpy())]\n",
    "        return []\n",
    "    \n",
    "    @xb.utility\n",
    "    def get_explained_instance(self):\n",
    "        return self.instance\n",
    "\n",
    "    @xb.utility\n",
    "    def get_neighborhood(self):\n",
    "        neighborhood_preds = list()\n",
    "        neighborhood_indices = list()\n",
    "        for idx, instance, _ in self.weighted_instances:\n",
    "            e = self.explain_instance(instance, self.model.predict_proba)\n",
    "            neighborhood_indices.append(idx)\n",
    "            pred = e.class_names[0] if e.predict_proba[0] > e.predict_proba[1] else e.class_names[1]\n",
    "            neighborhood_preds.append(pred)\n",
    "        return list(zip(neighborhood_indices, neighborhood_preds))\n",
    "\n",
    "    @xb.utility\n",
    "    def get_training_data(self):\n",
    "        return self.train\n",
    "\n",
    "    @xb.utility\n",
    "    def get_model_prediction(self):\n",
    "        return self.model.predict(self.train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = LimeExplainer(train, labels_train, rf, train.keys(), data.target_names, discretize_continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<lime.explanation.Explanation at 0x7fcb48641940>"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.explain_instance(test.iloc[0], rf.predict_proba, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "{('accuracy', nan),\n ('area', 2.589477453233665e+139),\n ('balance', nan),\n ('coverage', 2.8498848684316357e-07),\n ('furthest_distance', 0.16073684608677166),\n ('inverse_coverage', 3508913.6795562045)}"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred metrics: {'area', 'furthest_distance', 'balance', 'inverse_coverage', 'accuracy', 'coverage'}\n"
     ]
    }
   ],
   "source": [
    "exp.infer_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{('accuracy', nan),\n ('area', 2.589477453233665e+139),\n ('balance', nan),\n ('coverage', 2.8498848684316357e-07),\n ('furthest_distance', 0.16073684608677166),\n ('inverse_coverage', 3508913.6795562045)}"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliefang/PycharmProjects/lab21-XAI-benchmark/xaibenchmark/explainer.py:60: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ({'coverage'}, 'inverse_coverage', metric(lambda : 1 / self.coverage())),\n"
     ]
    },
    {
     "data": {
      "text/plain": "{('accuracy', nan),\n ('area', 2.589477453233665e+139),\n ('balance', nan),\n ('coverage', 0.0),\n ('furthest_distance', 0.0),\n ('inverse_coverage', inf)}"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.explain_instance(test.iloc[10], rf.predict_proba, num_features=10)\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred metrics: {'area', 'furthest_distance', 'balance', 'inverse_coverage', 'accuracy', 'coverage'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliefang/PycharmProjects/lab21-XAI-benchmark/xaibenchmark/explainer.py:60: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ({'coverage'}, 'inverse_coverage', metric(lambda : 1 / self.coverage())),\n"
     ]
    },
    {
     "data": {
      "text/plain": "{('accuracy', nan),\n ('area', 2.589477453233665e+139),\n ('balance', nan),\n ('coverage', 0.0),\n ('furthest_distance', 0.0),\n ('inverse_coverage', inf)}"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.infer_metrics()\n",
    "exp.report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[i for i, w in exp.weighted_instances][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "(108,)"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.train.iloc[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}