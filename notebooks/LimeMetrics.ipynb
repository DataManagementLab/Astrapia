{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# util\n",
    "from xaibenchmark import load_adult as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = la.load_csv_data('adult', root_path='../data')\n",
    "\n",
    "def preprocess(*data_df): \n",
    "    def process_single(df):\n",
    "        \n",
    "        cat_df = pd.get_dummies(df, columns=data.categorical_features.keys())\n",
    "        missing_cols = {cat+'_'+str(attr) for cat in data.categorical_features \\\n",
    "                        for attr in data.categorical_features[cat]} - set(cat_df.columns)\n",
    "        for c in missing_cols:\n",
    "            cat_df[c] = 0\n",
    "            \n",
    "        cont_idx = list(set(data.data.keys()) - set(data.categorical_features.keys()))\n",
    "        cat_idx = [cat+'_'+str(attr) for cat in data.categorical_features \\\n",
    "                   for attr in data.categorical_features[cat]]\n",
    "        idx = cont_idx + cat_idx\n",
    "        return cat_df[idx]\n",
    "        \n",
    "    # Preprocess function for one-hot encoding categorical data\n",
    "    return [process_single(df) for df in data_df]\n",
    "\n",
    "train, dev, test = preprocess(data.data, data.data_dev, data.data_test)\n",
    "labels_train, labels_dev, labels_test = data.target, data.target_dev, data.target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(n_estimators=500)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(train, labels_train.to_numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels_dev.to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.89      0.92      0.91      7406\n",
      "        >50K       0.72      0.64      0.68      2362\n",
      "\n",
      "    accuracy                           0.85      9768\n",
      "   macro avg       0.81      0.78      0.79      9768\n",
      "weighted avg       0.85      0.85      0.85      9768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report')\n",
    "print('{:->60}'.format(''))\n",
    "print(sklearn.metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xaibenchmark as xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimeExplainer(xb.Explainer):\n",
    "    \n",
    "    def __init__(self, train_data, train_labels, predict_fn, feature_names, target_names, categorical_features=None, discretize_continuous=True):\n",
    "        \n",
    "        self.explainer = lime.lime_tabular.LimeTabularExplainer(train_data, feature_names=feature_names,\n",
    "                                                   class_names=target_names, categorical_features=None,\n",
    "                                                   discretize_continuous=discretize_continuous)\n",
    "        self.train = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.predict_fn = predict_fn\n",
    "        self.kernel_width = np.sqrt(train_data.shape[1]) * .75\n",
    "\n",
    "    def explain_instance(self, instance, predictor, num_features=10):\n",
    "        self.explanation = self.explainer.explain_instance(instance, predictor, num_features=num_features)\n",
    "        self.instance = instance\n",
    "        self.weighted_instances = self.get_weighted_instances()\n",
    "\n",
    "        return self.explanation\n",
    "    \n",
    "    @xb.metric\n",
    "    def area(self):\n",
    "        \"\"\"\n",
    "        Area that is covered by the kernel in high dimension of the feature space.\n",
    "        \"\"\"\n",
    "        kernel_width = np.sqrt(self.train.shape[1]) * .75\n",
    "        kernel_dimension = self.train.shape[1]\n",
    "        return (kernel_width * np.sqrt(2*np.pi))**kernel_dimension\n",
    "\n",
    "    @xb.metric\n",
    "    def coverage(self):\n",
    "        \"\"\"\n",
    "        Proportion of instances covered in the area\n",
    "        \"\"\"\n",
    "        weighted_instances = self.weighted_instances\n",
    "        return sum([weight for _, weight in self.weighted_instances]) / len(self.weighted_instances)\n",
    "    \n",
    "    @xb.metric\n",
    "    def furthest_distance(self):\n",
    "        kernel_width = np.sqrt(self.train.shape[1]) * .75\n",
    "        def kernel(distance):\n",
    "            return np.sqrt(np.exp(-distance**2/kernel_width**2))\n",
    "        training_instances = self.train.to_numpy()\n",
    "        distance_instances = (self.distance(self.instance, instance) for instance in training_instances)\n",
    "        weighted_distances = (distance * kernel(distance) for distance in distance_instances)\n",
    "        return sum(weighted_distances)\n",
    "\n",
    "    @xb.metric\n",
    "    def accuracy(self):\n",
    "        \"\"\"\n",
    "        Proportion of instances in the explanation neighborhood that shares the same output label by the\n",
    "        explainer and the ML model\n",
    "        \"\"\"\n",
    "\n",
    "        ml_preds = self.predict_fn.predict_proba(self.train)[:, 1]\n",
    "        ml_preds = ml_preds > 0.5\n",
    "        exp_preds = [self.predict_instance_surrogate(instance) for instance, _ in self.weighted_instances]\n",
    "        exp_preds = np.array(exp_preds) > 0.5\n",
    "        return  (ml_preds == exp_preds).sum() / len(exp_preds)\n",
    "\n",
    "    @xb.metric\n",
    "    def balance(self):\n",
    "        \"\"\"\n",
    "        Proportion of instances in the explanation neighborhood that has been assigned label 1 by the\n",
    "        explanation model\n",
    "        \"\"\"\n",
    "        exp_preds = [self.predict_instance_surrogate(instance) for instance, _ in self.weighted_instances]\n",
    "        exp_preds = np.array(exp_preds) > 0.5\n",
    "        return exp_preds.sum() / len(exp_preds)\n",
    "\n",
    "\n",
    "    @xb.utility\n",
    "    def distance(self, x, y):\n",
    "        return np.linalg.norm(x-y)\n",
    "    \n",
    "    @xb.utility\n",
    "    def get_weighted_instances(self):     \n",
    "        if hasattr(self, 'explanation'):\n",
    "            kernel_width = np.sqrt(self.train.shape[1]) * .75\n",
    "            def kernel(distance):\n",
    "                return np.sqrt(np.exp(-distance**2/kernel_width**2))\n",
    "            return [(instance, kernel(self.distance(self.instance, instance))) \\\n",
    "                    for instance in self.train.to_numpy()]\n",
    "        return []\n",
    "    \n",
    "    @xb.utility\n",
    "    def get_explained_instance(self):\n",
    "        return self.instance\n",
    "\n",
    "    @xb.utility\n",
    "    def get_training_data(self):\n",
    "        return self.train\n",
    "\n",
    "    @xb.utility\n",
    "    def predict_instance_surrogate(self, instance):\n",
    "        return np.clip(self.explanation.intercept[1] + sum(weight * \\\n",
    "               ((instance - self.explainer.scaler.mean_)/self.explainer.scaler.scale_ )[idx] \\\n",
    "               for idx, weight in self.explanation.local_exp[1]), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = LimeExplainer(train, labels_train, rf, train.keys(), data.target_names, discretize_continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<lime.explanation.Explanation at 0x7fab787e66a0>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.explain_instance(test.iloc[0], rf.predict_proba, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{('accuracy', 0.8059053218093274),\n ('area', 2.589477453233665e+139),\n ('balance', 0.07366296670030273),\n ('coverage', 2.8498765237894297e-07),\n ('furthest_distance', 0.16073596211970273)}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred metrics: {'coverage', 'accuracy', 'inverse_coverage', 'area', 'balance', 'furthest_distance'}\n"
     ]
    }
   ],
   "source": [
    "exp.infer_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{('accuracy', 0.8059053218093274),\n ('area', 2.589477453233665e+139),\n ('balance', 0.07366296670030273),\n ('coverage', 2.8498765237894297e-07),\n ('furthest_distance', 0.16073596211970273),\n ('inverse_coverage', 3508923.9539063186)}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliefang/PycharmProjects/lab21-XAI-benchmark/xaibenchmark/explainer.py:60: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ({'coverage'}, 'inverse_coverage', metric(lambda : 1 / self.coverage())),\n"
     ]
    },
    {
     "data": {
      "text/plain": "{('accuracy', 0.8083183433510288),\n ('area', 2.589477453233665e+139),\n ('balance', 0.07353134734348264),\n ('coverage', 0.0),\n ('furthest_distance', 0.0),\n ('inverse_coverage', inf)}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.explain_instance(test.iloc[10], rf.predict_proba, num_features=10)\n",
    "exp.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred metrics: {'coverage', 'accuracy', 'inverse_coverage', 'area', 'balance', 'furthest_distance'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliefang/PycharmProjects/lab21-XAI-benchmark/xaibenchmark/explainer.py:60: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ({'coverage'}, 'inverse_coverage', metric(lambda : 1 / self.coverage())),\n"
     ]
    },
    {
     "data": {
      "text/plain": "{('accuracy', 0.8083183433510288),\n ('area', 2.589477453233665e+139),\n ('balance', 0.07353134734348264),\n ('coverage', 0.0),\n ('furthest_distance', 0.0),\n ('inverse_coverage', inf)}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.infer_metrics()\n",
    "exp.report()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}